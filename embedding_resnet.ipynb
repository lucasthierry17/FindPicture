{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask is pip installed\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does Feature Extraction Work?\n",
    "\n",
    "- **ResNet50** is a convolutional neural network (CNN) pretrained on the ImageNet dataset, which contains millions of images across thousands of categories.\n",
    "- It processes the image through multiple layers to generate a high-dimensional vector (embedding) that represents the image's features.\n",
    "- The embedding is a high-dimensional vector.\n",
    "- The layers in the network apply convolutional filters to the input image, capturing local patterns such as edges, textures, and simple shapes.\n",
    "- The pooling layers downsample the feature maps, reducing their dimensionality and making the model more computationally efficient.\n",
    "- Fully connected layers at the end would normally classify the image based on the extracted features.\n",
    "- For feature extraction, you use the output of the avg-pool layer.\n",
    "- The output is a 2048-dimensional vector that captures a summary of the image's high-level features.\n",
    "\n",
    "### What Does the Model Take into Consideration?\n",
    "\n",
    "- **Edges and corners**: Simple patterns at lower layers.\n",
    "- **Textures**: Patterns like fabric textures or foliage.\n",
    "- **Shapes and object parts**: Intermediate layers capture shapes and parts of objects.\n",
    "- **Objects and scenes**: Higher layers capture complex objects and their arrangements within scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model pre-trained on ImageNet (taktes around 2 sec)\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the preprocessing work and why is it necessary?\n",
    "\n",
    "- The image gets loaded and then preprocessed to match the format of the ResNet50 model.\n",
    "- This involves resizing the picture to 224x224 pixels, normalizing the pixel values, and adding an extra dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract embedding with error handling\n",
    "def extract_embedding(img_path, model):\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(224, 224))  # Load the image\n",
    "        img_data = image.img_to_array(img)  # Convert the image to an array\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)  # Preprocess the image to match the input format from ResNet50\n",
    "        embedding = model.predict(img_data)\n",
    "        return embedding.flatten()  # Return the flattened result\n",
    "    except Exception as error:\n",
    "        print(f\"Error processing {img_path}: {error}\")\n",
    "        return None\n",
    "\n",
    "# Function to get all image paths in the folder\n",
    "def get_image_paths(main_folder):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(main_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                image_paths.append(full_path)\n",
    "    return image_paths\n",
    "\n",
    "# Extract embeddings for all images in the directory with logging and progress bar\n",
    "def extract_and_store_embeddings(image_paths, model, batch_size=32):\n",
    "    @delayed\n",
    "    def process_batch(batch):\n",
    "        embeddings = []\n",
    "        valid_image_paths = []\n",
    "        for img_path in batch:\n",
    "            embedding = extract_embedding(img_path, model)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "                valid_image_paths.append(img_path)\n",
    "        return embeddings, valid_image_paths\n",
    "\n",
    "    # Create batches\n",
    "    batches = [image_paths[i:i + batch_size] for i in range(0, len(image_paths), batch_size)]\n",
    "\n",
    "    # Process batches in parallel\n",
    "    results = [process_batch(batch) for batch in batches]\n",
    "    results = compute(*results)\n",
    "\n",
    "    # Combine results\n",
    "    all_embeddings = np.concatenate([result[0] for result in results])\n",
    "    all_valid_image_paths = sum([result[1] for result in results], [])\n",
    "\n",
    "    if len(all_embeddings) == 0:\n",
    "        raise ValueError(\"No valid embeddings were extracted. Check the image paths and processing.\")\n",
    "    return all_embeddings, all_valid_image_paths\n",
    "\n",
    "# Function to find similar images\n",
    "def find_similar_images(input_img_path, model, embeddings, image_paths, top_n=5):\n",
    "    input_embedding = extract_embedding(input_img_path, model)\n",
    "    if input_embedding is None:\n",
    "        print(f\"Failed to extract embedding for input image: {input_img_path}\")\n",
    "        return []\n",
    "    similarities = cosine_similarity([input_embedding], embeddings)[0]\n",
    "    indices = np.argsort(similarities)[::-1][:top_n] # sort by similarity\n",
    "    similar_images = [(image_paths[i], similarities[i]) for i in indices]\n",
    "    return similar_images\n",
    "\n",
    "# Save embeddings and image paths\n",
    "def save_embeddings_and_paths(embeddings, image_paths, embeddings_path, image_paths_path):\n",
    "    with open(embeddings_path, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)  # Write embeddings in pickle file\n",
    "    with open(image_paths_path, 'wb') as f:\n",
    "        pickle.dump(image_paths, f)  # Write image_paths in pickle file\n",
    "\n",
    "# Load embeddings and image paths\n",
    "def load_embeddings_and_paths(embeddings_path, image_paths_path):\n",
    "    with open(embeddings_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    with open(image_paths_path, 'rb') as f:\n",
    "        image_paths = pickle.load(f)\n",
    "    return embeddings, image_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does the Similarity Calculation Work?\n",
    "\n",
    "- Cosine similarity measures the cosine of the angle between two vectors, providing a value between -1 (opposite) and 1 (the same).\n",
    "- Cosine similarity is useful for comparing high-dimensional vectors because it measures the orientation of the vectors rather than their magnitude.\n",
    "- This is beneficial because vectors might have different magnitudes but still represent similar content.\n",
    "\n",
    "### What is special about the cosine similarity?\n",
    "- focuses on the direction of the vectors, which means two vectors with the same direction but different magnitudes will be considered similar\n",
    "- cosine similarity remains effective because it normalizes the vectors to unit length before measuring the angle between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = 'C:\\\\Users\\\\lucas\\\\OneDrive - Hochschule Düsseldorf\\\\Uni_Drive\\\\DIV2k'\n",
    "embeddings_path = 'embeddings.pkl'\n",
    "image_paths_path = 'image_paths.pkl'\n",
    "\n",
    "if not os.path.exists(embeddings_path) or not os.path.exists(image_paths_path):\n",
    "    image_paths = get_image_paths(main_folder)\n",
    "    with ProgressBar():\n",
    "        embeddings, valid_image_paths = extract_and_store_embeddings(image_paths, model)\n",
    "    save_embeddings_and_paths(embeddings, valid_image_paths, embeddings_path, image_paths_path)\n",
    "else:\n",
    "    embeddings, valid_image_paths = load_embeddings_and_paths(embeddings_path, image_paths_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0495.png, Similarity: 0.62\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0294.png, Similarity: 0.59\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0163.png, Similarity: 0.58\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0763.png, Similarity: 0.55\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0258.png, Similarity: 0.52\n"
     ]
    }
   ],
   "source": [
    "# set paths and find \n",
    "input_img_path = \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Kemelion.jpg\"\n",
    "similar_images = find_similar_images(input_img_path, model, embeddings, valid_image_paths, top_n=5)\n",
    "\n",
    "# Display the input image\n",
    "input_image = Image.open(input_img_path)\n",
    "input_image.show()\n",
    "\n",
    "# Display similar images\n",
    "for img_path, similarity in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity: {similarity:.2f}\")\n",
    "    similar_image = Image.open(img_path)\n",
    "    similar_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code takes arouund 200 seconds for 1000 images. \n",
    "# this would be a time of 27 hours for 500.000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1424953  0.01408025 0.24473271 ... 0.16306274 0.18841374 0.58192897]\n",
      "The shape of the embedding is (901, 2048)\n",
      "The max value of the embedding array is 28.174291610717773 and the min is 0.0\n",
      "There are 901 image paths\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[1])\n",
    "print(f\"The shape of the embedding is {embeddings.shape}\")\n",
    "print(f\"The max value of the embedding array is {np.max(embeddings)} and the min is {np.min(embeddings)}\")\n",
    "print(f\"There are {len(valid_image_paths)} image paths\")\n",
    "assert(len(valid_image_paths) == embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the closest image to 5 input images. \n",
    "\n",
    "Steps in the computation (look further up to know how the embeddings are calculated)\n",
    "- Compute the mean value for each of the 2048 dimensions across all the input embeddings. \n",
    "\n",
    "### What happens if we input 5 completely random images?\n",
    "- the algorithm will search for an image whose embedding is closest to this averaged embedding\n",
    "- because the mean embedding is an aggregate of very diverse features, it might reuslt in selecting an image that contains some common elements or patterns found across the random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0054.png, Similarity: 0.6545925736427307\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_valid_HR\\DIV2K_valid_HR\\0808.png, Similarity: 0.6399517059326172\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0218.png, Similarity: 0.6277354955673218\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0426.png, Similarity: 0.621422529220581\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0589.png, Similarity: 0.6181597709655762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_embeddings(input_image_paths, model):\n",
    "    embeddings = [extract_embedding(img_path, model) for img_path in input_image_paths]\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def find_most_similar_to_group(input_image_paths, model, all_image_embeddings, all_image_paths, top_n=5):\n",
    "    mean_embedding = average_embeddings(input_image_paths, model)\n",
    "    similarities = cosine_similarity([mean_embedding], all_image_embeddings)[0]\n",
    "    indices = np.argsort(similarities)[::-1][:top_n]\n",
    "    similar_images = [(all_image_paths[i], similarities[i]) for i in indices]\n",
    "    return similar_images\n",
    "\n",
    "# Example usage\n",
    "input_image_paths = [\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Kemelion.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Krokodil.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Pferd_auf_Wiese.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\images.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Rasen.jpg\"\n",
    "]\n",
    "\n",
    "similar_images = find_most_similar_to_group(input_image_paths, model, embeddings, valid_image_paths)\n",
    "for img_path, similarity in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity: {similarity}\")\n",
    "    similar_image = Image.open(img_path)\n",
    "    similar_image.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other approach: Aggregated distance metric\n",
    "\n",
    "Compute the distance from each candidate image to each of the 5 input images, then aggregate these distances.\n",
    "The candidate image with the smallest aggregated distance is considered the closest match\n",
    "\n",
    "### How is the distance calculated in the aggregated distance metric?\n",
    "\n",
    "The aggregated distance metric is based on cosine similarity, which calculates the similarity between two vectors as the cosine of the angle between them\n",
    "Formula for the cosine similarity: (A * B) / (||A|| x ||B||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_valid_HR\\DIV2K_valid_HR\\0808.png, Aggregated Distance: 0.43927082419395447\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0054.png, Aggregated Distance: 0.4371415972709656\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0218.png, Aggregated Distance: 0.4256919324398041\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0426.png, Aggregated Distance: 0.41811156272888184\n",
      "Image: C:\\Users\\lucas\\OneDrive - Hochschule Düsseldorf\\Uni_Drive\\DIV2k\\DIV2K_train_HR\\DIV2K_train_HR\\0002.png, Aggregated Distance: 0.4162138104438782\n"
     ]
    }
   ],
   "source": [
    "def aggregate_distances(input_image_paths, model, all_image_embeddings, all_image_paths, top_n=5):\n",
    "    input_embeddings = [extract_embedding(img_path, model) for img_path in input_image_paths]\n",
    "    aggregated_distances = []\n",
    "\n",
    "    for i, candidate_embedding in enumerate(all_image_embeddings):\n",
    "        distances = [cosine_similarity([candidate_embedding], [input_embedding])[0][0] for input_embedding in input_embeddings]\n",
    "        aggregated_distance = np.mean(distances)\n",
    "        aggregated_distances.append((all_image_paths[i], aggregated_distance))\n",
    "\n",
    "    aggregated_distances.sort(key=lambda x: x[1], reverse=True)\n",
    "    return aggregated_distances[:top_n]\n",
    "\n",
    "# Example usage\n",
    "input_image_paths = [\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Kemelion.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Krokodil.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Pferd_auf_Wiese.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\images.jpg\",\n",
    "    \"C:\\\\Users\\\\lucas\\\\Downloads\\\\Rasen.jpg\"\n",
    "]\n",
    "\n",
    "similar_images = aggregate_distances(input_image_paths, model, embeddings, valid_image_paths)\n",
    "for img_path, distance in similar_images:\n",
    "    print(f\"Image: {img_path}, Aggregated Distance: {distance}\")\n",
    "    similar_image = Image.open(img_path)\n",
    "    similar_image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
